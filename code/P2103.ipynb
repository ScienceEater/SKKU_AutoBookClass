{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"P210422.ipynb","provenance":[{"file_id":"1D6krVG0PPJR2Je9g5eN_2h6JP73_NUXz","timestamp":1619050659265},{"file_id":"1d_7axqPO6iSbI6joKb5EAFKV2rPmXt6X","timestamp":1616054665801}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJbYXou6chZf","executionInfo":{"status":"ok","timestamp":1619050699204,"user_tz":-540,"elapsed":918,"user":{"displayName":"HyunMu KIM","photoUrl":"","userId":"17183843461782073344"}},"outputId":"f20aa291-e156-4630-8ef7-3d56153622a7"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Apr 22 00:18:18 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A33AjdYC8Kd_"},"source":["import glob\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==3\n","!pip install torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtwoHboFkVPB"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","from tqdm import tqdm, tqdm_notebook, notebook"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u63ciSGrkWvH"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)\n","device = torch.device(\"cuda:0\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWbbqt_ckZFT"},"source":["##GPU 사용 시\n","device = torch.device(\"cuda:0\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDKV2EU2kaeY"},"source":["torch.cuda.is_available()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMEUFoqSkb94"},"source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05KgOz1XkeYO"},"source":["from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","from gluonnlp.data import SentencepieceTokenizer\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qi-BEvIJkeUr"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jQgPcNmikeSM"},"source":["BASE_PATH = '/gdrive/MyDrive/etc/will'\n","# data_2019 = glob.glob(BASE_PATH + '/*') # + '*'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50ruXyvGkePx"},"source":["bertmodel, vocab = get_pytorch_kobert_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qfQ7Uv1keNV"},"source":["df = pd.read_csv(BASE_PATH+'/210422_second.csv', index_col=[0], encoding=\"utf-8\", dtype=np.object) #dtype=np.object)\n","df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u82XhA-gkeKl"},"source":["df['n100'].value_counts().plot(kind = 'bar')\n","print(df.groupby('n100').size().reset_index(name = 'count'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CiLIt177keIa"},"source":["g2 = df.groupby('n10')\n","g2.count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVuBPSoCkeFw"},"source":["tdf = df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZvjoEDwkeBJ"},"source":["from sklearn.model_selection import train_test_split\n","\n","trainX, testX, trainY, testY = train_test_split(tdf[['input']], tdf['n10'],\n","                                          test_size = 0.2,\n","                                          shuffle=True,\n","                                          stratify=tdf['n10'],\n","                                          random_state=2013)\n","print(len(trainX))\n","print(len(trainY))\n","print(len(testX))\n","print(len(testY))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76JPSIvfkd-T"},"source":["trainY.value_counts().plot(kind = 'bar')\n","print(trainY.size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3XrAM2jwkd76"},"source":["# 문장 추출\n","sentences = trainX['input'].values\n","# sentences[:10]\n","train_dataset = pd.DataFrame({'sentence': sentences, 'label': trainY}, columns=['sentence', 'label'])\n","\n","# 문장 추출\n","sentences = testX['input'].values\n","test_dataset = pd.DataFrame({'sentence': sentences, 'label': testY}, columns=['sentence', 'label'])\n","sentences[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQ3Ee5xSkd5a"},"source":["dataset_train = train_dataset.values\n","dataset_test = test_dataset.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LKKh-omkd3F"},"source":["tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","# tokenizer = get_tokenizer()\n","# bertmodel, vocab = get_pytorch_kobert_model()\n","# tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3y1irvZkd0u"},"source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = []\n","        self.labels = []\n","        \n","        for i in dataset:\n","            try:\n","                self.sentences.append(transform([i[sent_idx]]))\n","                self.labels.append(np.int32(i[label_idx]))\n","            except Exception as e:\n","                print(e)\n","                print(len(self.sentences), len(self.labels))\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fib98GGakdyZ"},"source":["## Setting parameters\n","max_len = 128 #128\n","batch_size = 2 #cuda out of memory ...\n","warmup_ratio = 0.1\n","num_epochs = 15\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  2e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuApybGpkdwM"},"source":["data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKW4CzM1kdtn"},"source":["train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=2)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzKe5-0EkdrC"},"source":["NUM_LABELS = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEY6t43Skdoj"},"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768, #768 #뭐여 이게.....뭔 조화여 dim이 달라지구로\n","                 #https://www.google.com/search?sxsrf=ALeKk01zaV-xIa8Z0CUMGLodIa4vl6Z9fg%3A1616147579994&lei=e3RUYIKtPMizmAXPr7D4BA&q=Mat1%20dim%201%20must%20match%20mat2%20dim%200&ved=2ahUKEwjC1qPzirzvAhXIGaYKHc8XDE8QsKwBKAB6BAgUEAE&biw=832&bih=720\n","                 num_classes=NUM_LABELS,     #9,#num_labels\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cBonJRxmefY"},"source":["model = BERTClassifier(bertmodel,  dr_rate=0.25).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSRKgZlUmecQ"},"source":["# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAnT9oQ0meZ5"},"source":["optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNEZmf4dmeXh"},"source":["t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Ll0-kv0meVL"},"source":["scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GsF4PyopmeS6"},"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwZ5CSngmeQe"},"source":["from time import gmtime, strftime\n","START_TIME = strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())\n","\n","SAVE_PATH = BASE_PATH+\"/bert_model_{}.pth\".format(START_TIME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUeHv5eTmeOH"},"source":["for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(notebook.tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    \n","    torch.save(model.state_dict(), SAVE_PATH)\n","    print(\"Model saved at {}\".format(SAVE_PATH))\n","\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(notebook.tqdm(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgMyIxO2meME"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UENhNX-BmeJZ"},"source":[""],"execution_count":null,"outputs":[]}]}